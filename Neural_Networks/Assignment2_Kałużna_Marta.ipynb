{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2_Kałużna_Marta.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGXgWugfJ0Vl",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 2\n",
        "\n",
        "**Submission deadline: last lab session before or on Thursday, 26.03.2020**\n",
        "\n",
        "**Points: 6 + 1 bonus points**\n",
        "\n",
        "## Submission instructions\n",
        "The class is held remotely. To sumbmit your solutions please save the notebook to your Google Drive, then:\n",
        "1. Rename it it to: Assignment2_Surname_FirstName\n",
        "2. Rerun the whole notebook `Runtime -> Restar and run all`\n",
        "3. Make a pinned revision `File->Save and pin revision`\n",
        "4. Share the notebook with your instructor using his `cs.uni.wroc.pl` email\n",
        "\n",
        "We will use the commenting system and video conferences to check and discuss the solutions.\n",
        "\n",
        "As always, please submit corrections using GitHub's Pull Requests."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfVDe-bMqVT_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiTEWD2oqW0Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZCM_hdELE04",
        "colab_type": "text"
      },
      "source": [
        "The code below contains a mock-up of a two-layer neural network. Fill in the code and manually set weights to solve the XOR problem.\n",
        "\n",
        "Please note: the shapes are set to be compatible with PyTorch's conventions:\n",
        "* a batch containing $N$ $D$-dimensional examples has shape $N\\times D$ (each example is a row!)\n",
        "* a weight matrix in a linear layer with $I$ inputs and $O$ outputs has shape $O \\times I$\n",
        "* a bias vector is a 1D vector. Please note that [broadcasting rules](https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html) allow us to think about it as a $1 \\times D` matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYEbCfbSpv5M",
        "colab_type": "code",
        "outputId": "d2739cd9-26f5-4ccc-a25b-3d75a046cda5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "# Let's define a XOR dataset\n",
        "\n",
        "# X will be matrix of N 2-dimensional inputs\n",
        "X = np.array(\n",
        "    [[0, 0],\n",
        "     [0, 1],\n",
        "     [1, 0],\n",
        "     [1, 1],\n",
        "    ], dtype=np.float32)\n",
        "# Y is a matrix of N numners - answers\n",
        "Y = np.array(\n",
        "    [[0],\n",
        "     [1],\n",
        "     [1],\n",
        "     [0],\n",
        "    ], dtype=np.float32)\n",
        "\n",
        "plt.scatter(X[:,0], X[:,1], c=Y[:,0], )\n",
        "plt.xlabel('X[0]')\n",
        "plt.ylabel('X[1]')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'X[1]')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkKTGGQSS1Lj/B7qld7UMS/7bAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rb3azMn929_I",
        "colab_type": "text"
      },
      "source": [
        "# Problem 1 [2p]\n",
        "\n",
        "Fill in the details of a forward pass, then manually set the weights and biases in the network to solve the 2D XOR task defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrrRuk6zLiF0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(x):\n",
        "    #return TODO\n",
        "    return 1/(1 + np.exp(-x))\n",
        "\n",
        "class SmallNet:\n",
        "    def __init__(self, in_features, num_hidden, dtype=np.float32):\n",
        "        self.W1 = np.zeros((in_features, num_hidden), dtype=dtype)\n",
        "        self.b1 = np.zeros((num_hidden,), dtype=dtype)\n",
        "        self.W2 = np.zeros((num_hidden, 1), dtype=dtype)\n",
        "        self.b2 = np.zeros((1,), dtype=dtype)\n",
        "        self.init_params()\n",
        "\n",
        "    def init_params(self):\n",
        "        # TODO Problem 2:\n",
        "        # set all parameters to small random values, e.g. from N(0, 0.5)\n",
        "        self.W1 = np.random.normal(0, 0.5, self.W1.shape)\n",
        "        self.b1 = np.random.normal(0, 0.5, self.b1.shape)\n",
        "        self.W2 = np.random.normal(0, 0.5, self.W2.shape)\n",
        "        self.b2 = np.random.normal(0, 0.5, self.b2.shape)\n",
        "\n",
        "    def forward(self, X, Y=None, do_backward=False):\n",
        "        # TODO Problem 1: Fill in details of forward propagation\n",
        "\n",
        "        # n = X.shape[0]\n",
        "        # Input to neurons in 1st layer\n",
        "        A1 = X @ self.W1 + self.b1  # n x num_hidden\n",
        "\n",
        "        # Outputs after the sigmoid non-linearity\n",
        "        O1 = sigmoid(A1)  # n x num_hidden\n",
        "\n",
        "        # Inputs to neuron in the second layer\n",
        "        A2 = O1 @ self.W2 + self.b2  # n x 1\n",
        "\n",
        "        # Outputs after the sigmoid non-linearity\n",
        "        O2 = sigmoid(A2) # n x 1\n",
        "\n",
        "        if Y is not None:\n",
        "            #loss = TODO cross-entropy loss\n",
        "            loss = - (Y * np.log(O2) + (1 - Y) * np.log(1 - O2))\n",
        "            # normalize loss by batch size\n",
        "            loss = loss.sum() / X.shape[0]\n",
        "        else:\n",
        "            loss = np.nan\n",
        "\n",
        "        if do_backward:\n",
        "            # TODO in Problem 2: fill in the gradient computation\n",
        "            # Please note, thate there is a correspondance between\n",
        "            # the forward and backward pass: with backward computations happening\n",
        "            # in reversed order. \n",
        "\n",
        "            # A2_grad is the gradient of loss with respect to A2\n",
        "            # Hint: there is a concise formula for the gradient \n",
        "            # of logistic sigmoid and cross-entropy loss\n",
        "            n = X.shape[0]\n",
        "            A2_grad = O2 - Y # n x 1\n",
        "            self.b2_grad = A2_grad.sum(0) / X.shape[0] \n",
        "            self.W2_grad = (A2_grad.T @ O1).T / X.shape[0]  # num_hidden x 1\n",
        "            O1_grad = A2_grad.dot(self.W2.T)  #X.shape[0] x num_hidden\n",
        "            A1_grad = O1_grad * O1 * (1 - O1) # n x num_hidden\n",
        "            self.b1_grad = A1_grad.sum(0) / X.shape[0]  # 1 x num_hidden\n",
        "            self.W1_grad = X.T @ A1_grad / X.shape[0]\n",
        "\n",
        "        return O2, loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJswvBk0oiIY",
        "colab_type": "code",
        "outputId": "d52fad46-3d98-4816-a4ae-e161029fc2b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "# TODO Problem 1:\n",
        "# Set by hand the weight values to solve the XOR problem\n",
        "\n",
        "net = SmallNet(2, 2, dtype=np.float64)\n",
        "net.W1 = np.random.normal(0, 0.5, net.W1.shape) # m x hidden\n",
        "net.W2 = np.random.normal(0, 0.5, net.W2.shape) # hidden x 1\n",
        "net.b1 = np.random.normal(0, 0.5, net.b1.shape)\n",
        "net.b2 = np.random.normal(0, 0.5, net.b2.shape)\n",
        "\n",
        "# Hint: since we use the logistic sigmoid activation, the weights may need to \n",
        "# be fairly large \n",
        "\n",
        "\n",
        "net.forward(X, Y, do_backward=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.68367299],\n",
              "        [0.69245376],\n",
              "        [0.6993888 ],\n",
              "        [0.70611098]]), 0.7751485252960455)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmxCi5Vl6_xB",
        "colab_type": "text"
      },
      "source": [
        "# Problem 2 [2p]\n",
        "\n",
        "1. Fill in the backward pass.\n",
        "2. Implement random initialization of network parameters.\n",
        "3. Using `float64` verify correctness of your backward pass."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSM5hgJ1mrhY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def check_grad(net, param_name, X, Y, eps=1e-5):\n",
        "    \"\"\"A gradient checking routine\"\"\"\n",
        "    \n",
        "    param = getattr(net, param_name)\n",
        "    param_flat_accessor = param.reshape(-1)\n",
        "\n",
        "    grad = np.empty_like(param)\n",
        "    grad_flat_accessor = grad.reshape(-1)\n",
        "\n",
        "    net.forward(X, Y, do_backward=True)\n",
        "    orig_grad = getattr(net, param_name + '_grad')\n",
        "    assert (param.shape == orig_grad.shape)\n",
        "\n",
        "    for i in range(param_flat_accessor.shape[0]):\n",
        "        orig_val = param_flat_accessor[i]\n",
        "        param_flat_accessor[i] = orig_val + eps\n",
        "        _, loss_positive = net.forward(X, Y)\n",
        "        param_flat_accessor[i] = orig_val - eps\n",
        "        _, loss_negative = net.forward(X, Y)\n",
        "        param_flat_accessor[i] = orig_val\n",
        "        grad_flat_accessor[i] = (loss_positive - loss_negative) / (2 * eps)\n",
        "    assert np.allclose(grad, orig_grad)\n",
        "    return grad, orig_grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTZu0jFEvgXF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = SmallNet(2, 2, dtype=np.float64)\n",
        "\n",
        "for param_name in ['W1', 'b1', 'W2', 'b2']:\n",
        "    check_grad(net, param_name, X, Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mUOs3cVvjM2",
        "colab_type": "text"
      },
      "source": [
        "# Problem 3 [2p]\n",
        "\n",
        "Fill in the details of batch gradient descent below, then train a network to solve 2D XOR problem.\n",
        "\n",
        "Then test the reliability of solving the 3D XOR task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nn2AAoZo0vjU",
        "colab_type": "code",
        "outputId": "cb867ec4-97ce-4e88-c55b-d84717030d72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "net = SmallNet(2, 10, dtype=np.float64)\n",
        "\n",
        "alpha = 1e-1\n",
        "\n",
        "for i in range(100000):\n",
        "    _, loss = net.forward(X, Y, do_backward=True)\n",
        "    if (i % 5000) == 0:\n",
        "        print(i, loss)\n",
        "    for param_name in ['W1', 'b1', 'W2', 'b2']:\n",
        "        param = getattr(net, param_name)\n",
        "        # Hint: use the construct `param[:]` to change the contents of the array!\n",
        "        # Doing instead `param = new_val` simply changes to what the variable\n",
        "        # param points to, without affecting the network!\n",
        "\n",
        "        #print(param)\n",
        "        #print(getattr(net, param_name + '_grad'))\n",
        "        param[:] = param[:] - alpha*getattr(net, param_name + '_grad')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.781109242783746\n",
            "5000 0.042318476722598274\n",
            "10000 0.008255007880009076\n",
            "15000 0.004314582892631719\n",
            "20000 0.0028761199124204954\n",
            "25000 0.0021422987974294393\n",
            "30000 0.001700374023238536\n",
            "35000 0.001406263871442924\n",
            "40000 0.0011969723969684155\n",
            "45000 0.0010407093165651661\n",
            "50000 0.0009197425863789303\n",
            "55000 0.0008234203498460487\n",
            "60000 0.0007449669714454986\n",
            "65000 0.0006798715019717068\n",
            "70000 0.0006250169834659948\n",
            "75000 0.0005781820456277922\n",
            "80000 0.0005377416759678499\n",
            "85000 0.0005024802254855994\n",
            "90000 0.00047147049181407534\n",
            "95000 0.00044399319073126515\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwpEjpkU1JvK",
        "colab_type": "code",
        "outputId": "a1f5a252-1132-4cfb-d3a2-3c7683a868f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "predictions, loss = net.forward(X, Y, do_backward=True)\n",
        "print(predictions), print(Y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3.19855781e-04]\n",
            " [9.99681728e-01]\n",
            " [9.99507269e-01]\n",
            " [5.46696578e-04]]\n",
            "[[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3NiL0ku2Unr",
        "colab_type": "text"
      },
      "source": [
        "Generate below data for a 3D XOR task. Try a few values of hidden layer size. Plot the reliability of training, i.e. how many trainings succeed for a given hiden layer size.\n",
        "\n",
        "What is easier to train: a smaller, or large network?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0ZMyHqz8xrC",
        "colab_type": "code",
        "outputId": "d530a016-391b-4edc-b189-3f71da524cbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        }
      },
      "source": [
        "X3 = np.array(\n",
        "    [[0, 0, 0],\n",
        "     [0, 0, 1],\n",
        "     [0, 1, 0],\n",
        "     [0, 1, 1],\n",
        "     [1, 0, 0],\n",
        "     [1, 0, 1],\n",
        "     [1, 1, 0],\n",
        "     [1, 1, 1],\n",
        "    ], dtype=np.float32)\n",
        "\n",
        "\n",
        "Y3 = np.array(\n",
        "    [[0],\n",
        "     [1],\n",
        "     [1],\n",
        "     [0],\n",
        "     [1],\n",
        "     [0],\n",
        "     [0],\n",
        "     [1]\n",
        "    ], dtype=np.float32)\n",
        "\n",
        "X3, Y3"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0., 0., 0.],\n",
              "        [0., 0., 1.],\n",
              "        [0., 1., 0.],\n",
              "        [0., 1., 1.],\n",
              "        [1., 0., 0.],\n",
              "        [1., 0., 1.],\n",
              "        [1., 1., 0.],\n",
              "        [1., 1., 1.]], dtype=float32), array([[0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [1.]], dtype=float32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yk8Rn44G2t61",
        "colab_type": "code",
        "outputId": "fa817434-d77e-4bc6-f399-758bf6390e32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for hidden_dim in [2, 3, 5, 10, 20]:\n",
        "    # TODO: run a few trainings and record the fraction of successful ones\n",
        "    print('Hidden layer size: ', hidden_dim)\n",
        "    net = SmallNet(3, hidden_dim, np.float64)\n",
        "    for i in range(100000):\n",
        "        preds, loss = net.forward(X3, Y3, do_backward=True)\n",
        "        if (i % 5000) == 0:\n",
        "            print(i, loss)\n",
        "        for param_name in ['W1', 'b1', 'W2', 'b2']:\n",
        "            param = getattr(net, param_name)\n",
        "            param[:] = param[:] - alpha * getattr(net, param_name + '_grad')\n",
        "    print('Prediction:', preds)\n",
        "    print('\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hidden layer size:  2\n",
            "0 0.7029745012942802\n",
            "5000 0.6931454095053089\n",
            "10000 0.69314391987953\n",
            "15000 0.6931396738473143\n",
            "20000 0.693115938075467\n",
            "25000 0.6929762387990066\n",
            "30000 0.6920966958966148\n",
            "35000 0.661549998621066\n",
            "40000 0.324213026543456\n",
            "45000 0.29711211317694747\n",
            "50000 0.29076278402186695\n",
            "55000 0.28798572759221897\n",
            "60000 0.2864369940143911\n",
            "65000 0.28545190529737163\n",
            "70000 0.28477119276793555\n",
            "75000 0.2842731464598752\n",
            "80000 0.2838931767364449\n",
            "85000 0.28359386479281873\n",
            "90000 0.2833520527173179\n",
            "95000 0.28315264919014627\n",
            "Prediction: [[0.00301243]\n",
            " [0.99793573]\n",
            " [0.99793573]\n",
            " [0.25112734]\n",
            " [0.99793583]\n",
            " [0.25112732]\n",
            " [0.25112732]\n",
            " [0.24979656]]\n",
            "\n",
            "\n",
            "Hidden layer size:  3\n",
            "0 0.7481120938560163\n",
            "5000 0.6891450494467932\n",
            "10000 0.0976201539275219\n",
            "15000 0.02085388424916263\n",
            "20000 0.01148035623275806\n",
            "25000 0.007913562153280025\n",
            "30000 0.006037650580724295\n",
            "35000 0.004881127723377794\n",
            "40000 0.004096828691061321\n",
            "45000 0.0035299617544723863\n",
            "50000 0.0031011025579142553\n",
            "55000 0.0027653089696957076\n",
            "60000 0.0024952421205128774\n",
            "65000 0.0022733168922269027\n",
            "70000 0.0020877083844282373\n",
            "75000 0.001930171879000245\n",
            "80000 0.0017947831843057173\n",
            "85000 0.0016771758483501318\n",
            "90000 0.0015740606987183024\n",
            "95000 0.0014829128825051764\n",
            "Prediction: [[8.88973558e-05]\n",
            " [9.98422805e-01]\n",
            " [9.98434823e-01]\n",
            " [2.23856389e-03]\n",
            " [9.98066604e-01]\n",
            " [1.82169525e-03]\n",
            " [1.86362019e-03]\n",
            " [9.99884589e-01]]\n",
            "\n",
            "\n",
            "Hidden layer size:  5\n",
            "0 0.696531444411567\n",
            "5000 0.19954148502883456\n",
            "10000 0.014475204665321902\n",
            "15000 0.00632238260426808\n",
            "20000 0.003934188524103105\n",
            "25000 0.0028263326229597254\n",
            "30000 0.0021940428500818667\n",
            "35000 0.0017875890440153188\n",
            "40000 0.0015052976572885201\n",
            "45000 0.001298287903522902\n",
            "50000 0.0011402414042626144\n",
            "55000 0.001015769434944162\n",
            "60000 0.0009152899529175364\n",
            "65000 0.0008325331738532724\n",
            "70000 0.0007632285693773524\n",
            "75000 0.0007043692424181795\n",
            "80000 0.0006537781562750841\n",
            "85000 0.0006098411879153858\n",
            "90000 0.000571336779331444\n",
            "95000 0.0005373237892648097\n",
            "Prediction: [[4.31646354e-04]\n",
            " [9.99439127e-01]\n",
            " [9.99356299e-01]\n",
            " [4.38922041e-04]\n",
            " [9.99493427e-01]\n",
            " [6.56352692e-04]\n",
            " [5.21499976e-04]\n",
            " [9.99704075e-01]]\n",
            "\n",
            "\n",
            "Hidden layer size:  10\n",
            "0 0.8589914904468069\n",
            "5000 0.6884220353027478\n",
            "10000 0.048302916714039704\n",
            "15000 0.012170027771875123\n",
            "20000 0.0064905132240951355\n",
            "25000 0.004325147866707993\n",
            "30000 0.0032076806502653477\n",
            "35000 0.0025330000725477175\n",
            "40000 0.0020843364028569596\n",
            "45000 0.001765750310121977\n",
            "50000 0.001528532940254523\n",
            "55000 0.0013454369596328675\n",
            "60000 0.0012000766064995385\n",
            "65000 0.001082028550139266\n",
            "70000 0.0009843576372966425\n",
            "75000 0.0009022767622822538\n",
            "80000 0.0008323786666935093\n",
            "85000 0.0007721744176484616\n",
            "90000 0.0007198049475761291\n",
            "95000 0.0006738545645625347\n",
            "Prediction: [[7.01545002e-04]\n",
            " [9.99260984e-01]\n",
            " [9.99334734e-01]\n",
            " [5.31012536e-04]\n",
            " [9.99228125e-01]\n",
            " [7.69474020e-04]\n",
            " [5.98812506e-04]\n",
            " [9.99712822e-01]]\n",
            "\n",
            "\n",
            "Hidden layer size:  20\n",
            "0 0.7076542612405075\n",
            "5000 0.6600384675533609\n",
            "10000 0.021109395384258163\n",
            "15000 0.007472896264273072\n",
            "20000 0.0043980650314511885\n",
            "25000 0.0030828790472795893\n",
            "30000 0.002360971899767757\n",
            "35000 0.0019073106750171938\n",
            "40000 0.0015968295086102775\n",
            "45000 0.0013714670621069\n",
            "50000 0.0012006958770453373\n",
            "55000 0.0010669698492990617\n",
            "60000 0.000959504184368418\n",
            "65000 0.0008713130089348808\n",
            "70000 0.0007976763362594944\n",
            "75000 0.00073529266591934\n",
            "80000 0.0006817847385085991\n",
            "85000 0.0006353981110085527\n",
            "90000 0.0005948102621098936\n",
            "95000 0.0005590057273372305\n",
            "Prediction: [[5.69323418e-04]\n",
            " [9.99461164e-01]\n",
            " [9.99447123e-01]\n",
            " [5.30190249e-04]\n",
            " [9.99429273e-01]\n",
            " [5.00105450e-04]\n",
            " [5.04951718e-04]\n",
            " [9.99550543e-01]]\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuaLEoV-9DLG",
        "colab_type": "text"
      },
      "source": [
        "# Problem 4 [1bp]\n",
        "\n",
        "Replace the first nonlinearity with the [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) activation function. Verify ho"
      ]
    }
  ]
}