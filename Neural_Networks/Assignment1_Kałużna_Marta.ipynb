{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment1_Kałużna_Marta.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAiECkYEaZn1",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 1\n",
        "\n",
        "## Important notes\n",
        "**Submission deadline:**\n",
        "* **Thursday, 12.03.2020**\n",
        "\n",
        "**Points: 13 + 2bp**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bb0zN1GvapSt",
        "colab_type": "text"
      },
      "source": [
        "This assignment is meant to test your skills in course pre-requisites:  Scientific Python programming and  Machine Learning. If it is hard, I strongly advise you to drop the course.\n",
        "\n",
        "Please use GitHub’s [pull requests](https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/about-pull-requests) and issues to send corrections!\n",
        "\n",
        "You can solve the assignment in any system you like, but we encourage you to try out [Google Colab](https://colab.research.google.com/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIBf-IF_ahTI",
        "colab_type": "text"
      },
      "source": [
        "## Assignment text\n",
        "1. **[1p]** Download data competition from a Kaggle competition on sentiment prediction from [[https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data)].  Keep only full sentences, i.e. for each `SenteceId` keep only the entry with the lowest `PhraseId`.  Use first 7000 sentences as a `train set` and the remaining 1529 sentences as the `test set`. \n",
        "\n",
        "2. **[1p]** Prepare the data for logistic regression:\n",
        "\tMap the sentiment scores $0,1,2,3,4$ to a probability of the sentence being by setting $p(\\textrm{positive}) = \\textrm{sentiment}/4$.\n",
        "\tBuild a dictionary of at most 20000 most frequent words.\n",
        "\n",
        "3. **[3p]** Treat each document as a bag of words. e.g. if the vocabulary is \n",
        "\t```\n",
        "\t0: the\n",
        "\t1: good\n",
        "\t2: movie\n",
        "\t3: is\n",
        "\t4: not\n",
        "\t5: a\n",
        "\t6: funny\n",
        "\t```\n",
        "\tThen the encodings can be:\n",
        "\t```\n",
        "\tgood:                           [0,1,0,0,0,0,0]\n",
        "\tnot good:                       [0,1,0,0,1,0,0] \n",
        "\tthe movie is not a funny movie: [1,0,2,1,1,1,1]\n",
        "\t```\n",
        "    Train a logistic regression model to predict the sentiment. Compute the correlation between the predicted probabilities and the sentiment. Record the most positive and negative words.\n",
        "    Please note that in this model each word gets its sentiment parameter $S_w$ and the score for a sentence is \n",
        "    $$\\text{score}(\\text{sentence}) = \\sum_{w\\text{ in sentence}}S_w$$\n",
        "\n",
        "4. **[3p]** Now prepare an encoding in which negation flips the sign of the following words. For instance for our vocabulary the encodings become:\n",
        "\t```\n",
        "\tgood:                           [0,1,0,0,0,0,0]\n",
        "\tnot good:                       [0,-1,0,0,1,0,0]\n",
        "\tnot not good:                   [0,1,0,0,0,0,0]\n",
        "\tthe movie is not a funny movie: [1,0,0,1,1,-1,-1]\n",
        "\t```\n",
        "\tFor best results, you will probably need to construct a list of negative words.\n",
        "\t\n",
        "\tAgain train a logistic regression classifier and compare the results to the Bag of Words approach.\n",
        "\t\n",
        "\tPlease note that this model still maintains a single parameter for each word, but now the sentence score is\n",
        "\t$$\\text{score}(\\text{sentence}) = \\sum_{w\\text{ in sentence}}-1^{\\text{count of negations preceeding }w}S_w$$\n",
        "\n",
        "5. **[5p]** Now also consider emphasizing words such as `very`. They can boost (multiply by a constant >1) the following words.\n",
        "\tImplement learning the modifying multiplier for negation and for emphasis. One way to do this is to introduce a model which has:\n",
        "\t- two modifiers, $N$ for negation and $E$ for emphasis\n",
        "\t- a sentiment score $S_w$ for each word \n",
        "And score each sentence as:\n",
        "$$\\text{score}(\\text{sentence}) = \\sum_{w\\text{ in sentence}}N^{\\text{\\#negs prec. }w}E^{\\text{\\#emphs prec. }w}S_w$$\n",
        "\n",
        "You will need to implement a custom logistic regression model to support it.\n",
        "\n",
        "6. **[2pb]** Propose, implement, and evaluate an extension to the above model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xembZybw5jZ3",
        "colab_type": "text"
      },
      "source": [
        "## 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XreUjdpjCxe7",
        "colab_type": "text"
      },
      "source": [
        "Kaggle data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCqKuAZQ_Brg",
        "colab_type": "code",
        "outputId": "c710e907-52b8-47d7-d3af-a0975db53ee3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "root_path = 'gdrive/My Drive/'\n",
        "\n",
        "# Install Kaggle library\n",
        "!pip install -q kaggle\n",
        "#!cp ~/kaggle.json ~/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AR_DYlLACBs",
        "colab_type": "code",
        "outputId": "89b5d855-80bb-41d4-b451-8bd107915e61",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 75
        }
      },
      "source": [
        "# Upload kaggle API key file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7cdf2c37-b22b-431f-a5f6-496579b7eb51\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-7cdf2c37-b22b-431f-a5f6-496579b7eb51\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle (1).json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67vjaIa9_aSU",
        "colab_type": "code",
        "outputId": "d061da95-bf05-42e5-a308-931efd8e165d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " gdrive\t\t    kaggle.json   sampleSubmission.csv\t train.tsv.zip\n",
            "'kaggle (1).json'   sample_data   test.tsv.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ot_TUfcvOdAg",
        "colab_type": "code",
        "outputId": "cf595241-a6f0-41af-b877-33e10e9dd6a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!ls ~/.kaggle\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoFZyB0T_zhQ",
        "colab_type": "code",
        "outputId": "5cc4b701-e0fa-408a-8ffa-a80ca1f87aa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!kaggle competitions download -c sentiment-analysis-on-movie-reviews"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "train.tsv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "test.tsv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "sampleSubmission.csv: Skipping, found more recently modified local copy (use --force to force download)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuqGJbfuAMiQ",
        "colab_type": "code",
        "outputId": "641df4ca-40d4-4c5d-c4d9-445ff68601b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " gdrive\t\t    kaggle.json   sampleSubmission.csv\t train.tsv.zip\n",
            "'kaggle (1).json'   sample_data   test.tsv.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRa6B2KCAVIy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Import the test and train datasets into pandas dataframe\n",
        "df_train = pd.read_csv('train.tsv.zip', header = 0, compression = 'zip', sep = '\\t', quotechar = '\"')\n",
        "df_test = pd.read_csv('test.tsv.zip', header = 0, compression = 'zip', sep = '\\t', quotechar = '\"')\n",
        "df_sample = pd.read_csv('sampleSubmission.csv', header = 0, sep = ',', quotechar = '\"')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjMhNgDVEtnR",
        "colab_type": "code",
        "outputId": "e3ce2f41-ff7c-4d95-e09c-4305b03ce090",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PhraseId</th>\n",
              "      <th>SentenceId</th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>A series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PhraseId  ...  Sentiment\n",
              "0         1  ...          1\n",
              "1         2  ...          2\n",
              "2         3  ...          2\n",
              "3         4  ...          2\n",
              "4         5  ...          2\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NL2qG1rQC8c7",
        "colab_type": "text"
      },
      "source": [
        "Only full sentences - the entries with the lowest *PhraseId*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMD9X1oQFByO",
        "colab_type": "code",
        "outputId": "dc181403-005d-47f1-d1b1-7d4f789a9f3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "df_train_full = df_train.loc[df_train.groupby(by='SentenceId')['PhraseId'].idxmin()].reset_index(drop=True)\n",
        "df_train_full"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PhraseId</th>\n",
              "      <th>SentenceId</th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>This quiet , introspective and entertaining in...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>82</td>\n",
              "      <td>3</td>\n",
              "      <td>Even fans of Ismail Merchant 's work , I suspe...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>117</td>\n",
              "      <td>4</td>\n",
              "      <td>A positively thrilling combination of ethnogra...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>157</td>\n",
              "      <td>5</td>\n",
              "      <td>Aggressive self-glorification and a manipulati...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8524</th>\n",
              "      <td>155985</td>\n",
              "      <td>8540</td>\n",
              "      <td>... either you 're willing to go with this cla...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8525</th>\n",
              "      <td>155998</td>\n",
              "      <td>8541</td>\n",
              "      <td>Despite these annoyances , the capable Claybur...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8526</th>\n",
              "      <td>156022</td>\n",
              "      <td>8542</td>\n",
              "      <td>-LRB- Tries -RRB- to parody a genre that 's al...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8527</th>\n",
              "      <td>156032</td>\n",
              "      <td>8543</td>\n",
              "      <td>The movie 's downfall is to substitute plot fo...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8528</th>\n",
              "      <td>156040</td>\n",
              "      <td>8544</td>\n",
              "      <td>The film is darkly atmospheric , with Herrmann...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8529 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      PhraseId  ...  Sentiment\n",
              "0            1  ...          1\n",
              "1           64  ...          4\n",
              "2           82  ...          1\n",
              "3          117  ...          3\n",
              "4          157  ...          1\n",
              "...        ...  ...        ...\n",
              "8524    155985  ...          2\n",
              "8525    155998  ...          2\n",
              "8526    156022  ...          1\n",
              "8527    156032  ...          1\n",
              "8528    156040  ...          2\n",
              "\n",
              "[8529 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWdqPG0ZDQ3Z",
        "colab_type": "text"
      },
      "source": [
        "**Train set** - first 7000 sentences, **test set** - remaining 1529."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZ5tF_UTJnuc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_full = df_train_full.head(7000)\n",
        "test_full = df_train_full.tail(1529)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SCYK7Mr0Wv-",
        "colab_type": "text"
      },
      "source": [
        "## 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvIh78atDqvr",
        "colab_type": "text"
      },
      "source": [
        "Preparing the data for logistic regression.    \n",
        "Probability: $p(\\textrm{positive}) = \\textrm{sentiment}/4$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNPgxnWVL-yZ",
        "colab_type": "code",
        "outputId": "2843218e-a955-4312-ecba-f97a436016cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "train_full.assign(Probability = train_full['Sentiment']/4)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PhraseId</th>\n",
              "      <th>SentenceId</th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Probability</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>This quiet , introspective and entertaining in...</td>\n",
              "      <td>4</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>82</td>\n",
              "      <td>3</td>\n",
              "      <td>Even fans of Ismail Merchant 's work , I suspe...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>117</td>\n",
              "      <td>4</td>\n",
              "      <td>A positively thrilling combination of ethnogra...</td>\n",
              "      <td>3</td>\n",
              "      <td>0.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>157</td>\n",
              "      <td>5</td>\n",
              "      <td>Aggressive self-glorification and a manipulati...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6995</th>\n",
              "      <td>130088</td>\n",
              "      <td>7007</td>\n",
              "      <td>Snoots will no doubt rally to its cause , trot...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6996</th>\n",
              "      <td>130131</td>\n",
              "      <td>7008</td>\n",
              "      <td>It 's better suited for the history or biograp...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6997</th>\n",
              "      <td>130161</td>\n",
              "      <td>7009</td>\n",
              "      <td>Buries an interesting storyline</td>\n",
              "      <td>2</td>\n",
              "      <td>0.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6998</th>\n",
              "      <td>130181</td>\n",
              "      <td>7010</td>\n",
              "      <td>This one is a few bits funnier than Malle 's d...</td>\n",
              "      <td>3</td>\n",
              "      <td>0.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6999</th>\n",
              "      <td>130214</td>\n",
              "      <td>7011</td>\n",
              "      <td>The film has several strong performances .</td>\n",
              "      <td>3</td>\n",
              "      <td>0.75</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7000 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      PhraseId  SentenceId  ... Sentiment  Probability\n",
              "0            1           1  ...         1         0.25\n",
              "1           64           2  ...         4         1.00\n",
              "2           82           3  ...         1         0.25\n",
              "3          117           4  ...         3         0.75\n",
              "4          157           5  ...         1         0.25\n",
              "...        ...         ...  ...       ...          ...\n",
              "6995    130088        7007  ...         1         0.25\n",
              "6996    130131        7008  ...         1         0.25\n",
              "6997    130161        7009  ...         2         0.50\n",
              "6998    130181        7010  ...         3         0.75\n",
              "6999    130214        7011  ...         3         0.75\n",
              "\n",
              "[7000 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYMHQKrX0cac",
        "colab_type": "text"
      },
      "source": [
        "20000 most frequent words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdcayJMYNQ_G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GleYJJSESjnh",
        "colab_type": "code",
        "outputId": "c67a8225-0868-4234-aecb-cb83873d0e49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vec = CountVectorizer()\n",
        "bag_of_words = vec.fit_transform(train_full['Phrase'])\n",
        "sum_words = bag_of_words.sum(axis = 0)\n",
        "words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "most_freq = dict(words_freq[:20000])\n",
        "len(words_freq)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13796"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYRXRD7XEwv2",
        "colab_type": "text"
      },
      "source": [
        "There are only 13796 words, so I'm going to show 50 most common words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6nUFk4tKvW4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#most_freq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOEHM_pFYui_",
        "colab_type": "code",
        "outputId": "3c873dda-07cc-4372-c585-05a27acd99c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "pd.DataFrame.from_dict(most_freq, orient = 'index')[:50]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>the</th>\n",
              "      <td>5942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>and</th>\n",
              "      <td>3684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>of</th>\n",
              "      <td>3605</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>to</th>\n",
              "      <td>2495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>is</th>\n",
              "      <td>2088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>it</th>\n",
              "      <td>1996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>that</th>\n",
              "      <td>1573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>in</th>\n",
              "      <td>1559</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>as</th>\n",
              "      <td>1044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>but</th>\n",
              "      <td>960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>film</th>\n",
              "      <td>947</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>with</th>\n",
              "      <td>929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>for</th>\n",
              "      <td>832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>movie</th>\n",
              "      <td>821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>this</th>\n",
              "      <td>803</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>an</th>\n",
              "      <td>782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>its</th>\n",
              "      <td>767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>you</th>\n",
              "      <td>692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>be</th>\n",
              "      <td>539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>on</th>\n",
              "      <td>537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>not</th>\n",
              "      <td>495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>one</th>\n",
              "      <td>463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>by</th>\n",
              "      <td>453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>more</th>\n",
              "      <td>437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>about</th>\n",
              "      <td>436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>are</th>\n",
              "      <td>430</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>has</th>\n",
              "      <td>429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>like</th>\n",
              "      <td>407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>at</th>\n",
              "      <td>403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>than</th>\n",
              "      <td>399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>all</th>\n",
              "      <td>398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>from</th>\n",
              "      <td>386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>have</th>\n",
              "      <td>373</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>his</th>\n",
              "      <td>362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>so</th>\n",
              "      <td>353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>or</th>\n",
              "      <td>304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>if</th>\n",
              "      <td>302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lrb</th>\n",
              "      <td>296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>rrb</th>\n",
              "      <td>296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>story</th>\n",
              "      <td>291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>what</th>\n",
              "      <td>289</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>out</th>\n",
              "      <td>280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>there</th>\n",
              "      <td>271</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>who</th>\n",
              "      <td>264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>up</th>\n",
              "      <td>257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>does</th>\n",
              "      <td>249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>too</th>\n",
              "      <td>249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>good</th>\n",
              "      <td>240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>just</th>\n",
              "      <td>236</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>into</th>\n",
              "      <td>234</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          0\n",
              "the    5942\n",
              "and    3684\n",
              "of     3605\n",
              "to     2495\n",
              "is     2088\n",
              "it     1996\n",
              "that   1573\n",
              "in     1559\n",
              "as     1044\n",
              "but     960\n",
              "film    947\n",
              "with    929\n",
              "for     832\n",
              "movie   821\n",
              "this    803\n",
              "an      782\n",
              "its     767\n",
              "you     692\n",
              "be      539\n",
              "on      537\n",
              "not     495\n",
              "one     463\n",
              "by      453\n",
              "more    437\n",
              "about   436\n",
              "are     430\n",
              "has     429\n",
              "like    407\n",
              "at      403\n",
              "than    399\n",
              "all     398\n",
              "from    386\n",
              "have    373\n",
              "his     362\n",
              "so      353\n",
              "or      304\n",
              "if      302\n",
              "lrb     296\n",
              "rrb     296\n",
              "story   291\n",
              "what    289\n",
              "out     280\n",
              "there   271\n",
              "who     264\n",
              "up      257\n",
              "does    249\n",
              "too     249\n",
              "good    240\n",
              "just    236\n",
              "into    234"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJ8bQ6700xEX",
        "colab_type": "text"
      },
      "source": [
        "## 3.\n",
        "$$\\text{score}(\\text{sentence}) = \\sum_{w\\text{ in sentence}}S_w$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wi_zvwcpSN3z",
        "colab_type": "text"
      },
      "source": [
        "Bag of words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfTxl6d1ctvf",
        "colab_type": "code",
        "outputId": "5d88955e-d691-400f-c6cc-82b2e87cb757",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "bag_of_words.toarray()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suVr-P8sNeVZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXeAT6BNSHNo",
        "colab_type": "code",
        "outputId": "b5ed8551-ffa3-4bbf-a491-783143de46c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.unique(bag_of_words.toarray())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6, 7])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUNuVsS7SUD8",
        "colab_type": "text"
      },
      "source": [
        "Training a logistic regression model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaXxypfk2tFT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FsMwzvQUas8",
        "colab_type": "code",
        "outputId": "10422490-03ed-4244-81c8-9374697e46ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "source": [
        "text_clf = Pipeline([('vect', CountVectorizer()), ('clf', LogisticRegression())])\n",
        "text_clf.fit(train_full['Phrase'], train_full['Sentiment'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('vect',\n",
              "                 CountVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
              "                                 input='content', lowercase=True, max_df=1.0,\n",
              "                                 max_features=None, min_df=1,\n",
              "                                 ngram_range=(1, 1), preprocessor=None,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, vocabulary=None)),\n",
              "                ('clf',\n",
              "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
              "                                    fit_intercept=True, intercept_scaling=1,\n",
              "                                    l1_ratio=None, max_iter=100,\n",
              "                                    multi_class='auto', n_jobs=None,\n",
              "                                    penalty='l2', random_state=None,\n",
              "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                                    warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvP9VhYAdjzr",
        "colab_type": "code",
        "outputId": "936e8efb-217f-4105-e984-dff352eb0273",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predicted = text_clf.predict(test_full['Phrase'])\n",
        "np.mean(predicted == test_full['Sentiment'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.39045127534336166"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0M3mzt6Fb8M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cm1 = confusion_matrix(test_full['Sentiment'], predicted)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQ1P-w530mut",
        "colab_type": "text"
      },
      "source": [
        "Correlation between the predicted probabilities and the sentiment - I think I didn't understand the task, so I did something but I'm not sure if it makes sense."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yykoRWm20o09",
        "colab_type": "code",
        "outputId": "1f1a3f18-c7ee-4c8f-d17d-df4a45ee1848",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "lr = LogisticRegression()\n",
        "lr = lr.fit(bag_of_words, train_full['Sentiment'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCu5KxfpFpXm",
        "colab_type": "code",
        "outputId": "59b786ae-2211-434b-c48f-185bfba38624",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "np.corrcoef(lr.predict_proba(bag_of_words), [0,1,2,3,4])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.        , -0.32714686,  0.98207972, ..., -0.10528025,\n",
              "        -0.33783629, -0.34560485],\n",
              "       [-0.32714686,  1.        , -0.2395563 , ..., -0.21517063,\n",
              "         0.06252968,  0.78522535],\n",
              "       [ 0.98207972, -0.2395563 ,  1.        , ..., -0.19301655,\n",
              "        -0.39813793, -0.37080915],\n",
              "       ...,\n",
              "       [-0.10528025, -0.21517063, -0.19301655, ...,  1.        ,\n",
              "         0.94975275,  0.29495314],\n",
              "       [-0.33783629,  0.06252968, -0.39813793, ...,  0.94975275,\n",
              "         1.        ,  0.50909045],\n",
              "       [-0.34560485,  0.78522535, -0.37080915, ...,  0.29495314,\n",
              "         0.50909045,  1.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YI3rcY40yQ2",
        "colab_type": "text"
      },
      "source": [
        "The most positive and negative words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eNvsq0a011b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# indices of the sorted cofficients\n",
        "coeff = lr.coef_\n",
        "inds_neg = np.argsort(coeff[[0]])\n",
        "inds_pos = np.argsort(coeff[[4]])\n",
        "feature_names = np.array(vec.get_feature_names())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkrkxu8X47aN",
        "colab_type": "code",
        "outputId": "11411e15-1068-499b-e197-d82689bb7ec9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "# negative (highest coeffs for sentiment = 0)\n",
        "for i in range(25):\n",
        "    print(feature_names[inds_neg[0,-i]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "best\n",
            "worst\n",
            "stupid\n",
            "devoid\n",
            "dull\n",
            "terrible\n",
            "disappointment\n",
            "horrible\n",
            "unpleasant\n",
            "mess\n",
            "bad\n",
            "incoherent\n",
            "insulting\n",
            "crap\n",
            "unbearably\n",
            "ugly\n",
            "awful\n",
            "purpose\n",
            "fails\n",
            "poor\n",
            "lazy\n",
            "generic\n",
            "horribly\n",
            "morbid\n",
            "snooze\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTtM0RE35KgM",
        "colab_type": "code",
        "outputId": "72210404-b2fd-49fe-d74a-62f36a49d6dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "# positive (highest coeffs for sentiment = 4)\n",
        "for i in range(25):\n",
        "    print(feature_names[inds_pos[0,-i]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "less\n",
            "masterpiece\n",
            "beautifully\n",
            "best\n",
            "captivating\n",
            "remarkable\n",
            "entertaining\n",
            "moving\n",
            "years\n",
            "dazzling\n",
            "riveting\n",
            "beautiful\n",
            "powerful\n",
            "wonderful\n",
            "fun\n",
            "brilliant\n",
            "fantastic\n",
            "thoughtful\n",
            "amazing\n",
            "rare\n",
            "masterful\n",
            "enjoyable\n",
            "impressive\n",
            "imax\n",
            "delights\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wydPgNczCTUU",
        "colab_type": "text"
      },
      "source": [
        "## 4.\n",
        "Encoding in which negation flips the sign of the following words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKuf1V2b9qNC",
        "colab_type": "text"
      },
      "source": [
        "$$\\text{score}(\\text{sentence}) = \\sum_{w\\text{ in sentence}}-1^{\\text{count of negations preceeding }w}S_w$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6w7RKdELrSAN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def df(text, col_names):\n",
        "    return pd.DataFrame(np.zeros((len(text), len(col_names))), columns = col_names)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wv7_V0R-sxsZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from string import punctuation\n",
        "\n",
        "def my_tokenizer(text):\n",
        "    text_lst = text.split()\n",
        "    for word in text_lst:\n",
        "        text_lst[text_lst.index(word)] = word.lower().strip(punctuation)\n",
        "    for word in text_lst:\n",
        "        if word is '':\n",
        "            text_lst.remove(word)\n",
        "    return text_lst"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJCZaz2cylTD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "full_lst = []\n",
        "for sentence in train_full['Phrase'].values:\n",
        "    full_lst.append(my_tokenizer(sentence))\n",
        "\n",
        "lst_scores = []\n",
        "for x in full_lst:\n",
        "    lst_scores.append([1]*len(x))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkLAB-fG17uD",
        "colab_type": "code",
        "outputId": "9d6cb4e7-c34d-4318-f8fc-98aa39fa83b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "one_lst = sum(full_lst, [])\n",
        "one_lst = list(set(one_lst))\n",
        "one_lst.sort()\n",
        "one_lst"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '1',\n",
              " '1.2',\n",
              " '1.8',\n",
              " '10',\n",
              " '10-course',\n",
              " '10-year-old',\n",
              " '100',\n",
              " '100-minute',\n",
              " '100-year',\n",
              " '101',\n",
              " '102-minute',\n",
              " '103-minute',\n",
              " '104',\n",
              " '105',\n",
              " '10th',\n",
              " '10th-grade',\n",
              " '11',\n",
              " '110',\n",
              " '112-minute',\n",
              " '12',\n",
              " '12-year-old',\n",
              " '120',\n",
              " '127',\n",
              " '129-minute',\n",
              " '12th',\n",
              " '13',\n",
              " '13th',\n",
              " '14-year-old',\n",
              " '140',\n",
              " '146',\n",
              " '15',\n",
              " '15th',\n",
              " '163',\n",
              " '168-minute',\n",
              " '170',\n",
              " '1790',\n",
              " '18',\n",
              " '18-year-old',\n",
              " '1899',\n",
              " '1915',\n",
              " '1920',\n",
              " '1930s',\n",
              " '1933',\n",
              " '1937',\n",
              " '1938',\n",
              " '1940s',\n",
              " '1950',\n",
              " '1950s',\n",
              " '1952',\n",
              " '1953',\n",
              " '1957',\n",
              " '1958',\n",
              " '1959',\n",
              " '1960s',\n",
              " '1962',\n",
              " '1970',\n",
              " '1970s',\n",
              " '1971',\n",
              " '1972',\n",
              " '1973',\n",
              " '1975',\n",
              " '1980',\n",
              " '1980s',\n",
              " '1984',\n",
              " '1986',\n",
              " '1989',\n",
              " '1990',\n",
              " '1992',\n",
              " '1994',\n",
              " '1995',\n",
              " '1997',\n",
              " '1999',\n",
              " '19th',\n",
              " '19th-century',\n",
              " '1\\\\/2',\n",
              " '2',\n",
              " '20',\n",
              " '2000',\n",
              " '2001',\n",
              " '2002',\n",
              " '20th',\n",
              " '20th-century',\n",
              " '21\\\\/2',\n",
              " '21st',\n",
              " '22',\n",
              " '22-year-old',\n",
              " '24-and-unders',\n",
              " '2455',\n",
              " '25',\n",
              " '26',\n",
              " '26-year-old',\n",
              " '270',\n",
              " '295',\n",
              " '2\\\\/3',\n",
              " '3',\n",
              " '3-d',\n",
              " '3-year-olds',\n",
              " '30',\n",
              " '30-year',\n",
              " '300',\n",
              " '3000',\n",
              " '3\\\\/4th',\n",
              " '3d',\n",
              " '4',\n",
              " '40',\n",
              " '42',\n",
              " '451',\n",
              " '48',\n",
              " '4\\\\/5ths',\n",
              " '4ever',\n",
              " '4w',\n",
              " '5',\n",
              " '50-million',\n",
              " '50-something',\n",
              " '50-year',\n",
              " '50-year-old',\n",
              " '50s',\n",
              " '51',\n",
              " '51st',\n",
              " '53',\n",
              " '6',\n",
              " '6-year-old',\n",
              " '60',\n",
              " '60-second',\n",
              " '60s',\n",
              " '65',\n",
              " '65-minute',\n",
              " '65-year-old',\n",
              " '65th',\n",
              " '7',\n",
              " '70s',\n",
              " '71',\n",
              " '72',\n",
              " '72-year-old',\n",
              " '75',\n",
              " '75-minute',\n",
              " '77',\n",
              " '78',\n",
              " '7th',\n",
              " '7th-century',\n",
              " '8',\n",
              " '8-10',\n",
              " '80',\n",
              " '80-minute',\n",
              " '80s',\n",
              " '83',\n",
              " '84',\n",
              " '85-minute',\n",
              " '86',\n",
              " '87',\n",
              " '88-minute',\n",
              " '89',\n",
              " '8th',\n",
              " '9',\n",
              " '9-11',\n",
              " '90',\n",
              " '90-minute',\n",
              " '90-plus',\n",
              " '90s',\n",
              " '91-minute',\n",
              " '93',\n",
              " '94',\n",
              " '95',\n",
              " '95-minute',\n",
              " '99-minute',\n",
              " '9\\\\/11',\n",
              " 'a',\n",
              " 'a-bornin',\n",
              " 'a-knocking',\n",
              " 'a-list',\n",
              " 'a-team',\n",
              " 'a.c',\n",
              " 'a.s',\n",
              " 'aaliyah',\n",
              " 'aan',\n",
              " 'abandon',\n",
              " 'abandoned',\n",
              " 'abbass',\n",
              " 'abbott',\n",
              " 'abbreviated',\n",
              " 'abc',\n",
              " 'abderrahmane',\n",
              " 'abdul',\n",
              " 'abel',\n",
              " 'abhorrent',\n",
              " 'abiding',\n",
              " 'abilities',\n",
              " 'ability',\n",
              " 'abject',\n",
              " 'able',\n",
              " 'ably',\n",
              " 'aboriginal',\n",
              " 'aborted',\n",
              " 'abound',\n",
              " 'about',\n",
              " 'above',\n",
              " 'above-average',\n",
              " 'abrasive',\n",
              " 'abridged',\n",
              " 'abrupt',\n",
              " 'absence',\n",
              " 'absent',\n",
              " 'absolute',\n",
              " 'absolutely',\n",
              " 'absorb',\n",
              " 'absorbed',\n",
              " 'absorbing',\n",
              " 'absorbs',\n",
              " 'abstract',\n",
              " 'absurd',\n",
              " 'absurdist',\n",
              " 'absurdities',\n",
              " 'absurdity',\n",
              " 'absurdly',\n",
              " 'abundant',\n",
              " 'abuse',\n",
              " 'abused',\n",
              " 'abysmal',\n",
              " 'abysmally',\n",
              " 'academic',\n",
              " 'academy',\n",
              " 'accent',\n",
              " 'accents',\n",
              " 'accept',\n",
              " 'acceptable',\n",
              " 'acceptance',\n",
              " 'accepting',\n",
              " 'accepts',\n",
              " 'access',\n",
              " 'accessibility',\n",
              " 'accessible',\n",
              " 'accident',\n",
              " 'accident-prone',\n",
              " 'accidental',\n",
              " 'acclaimed',\n",
              " 'accommodate',\n",
              " 'accomodates',\n",
              " 'accompanies',\n",
              " 'accompanying',\n",
              " 'accomplish',\n",
              " 'accomplished',\n",
              " 'accomplishes',\n",
              " 'accomplishment',\n",
              " 'accomplishments',\n",
              " 'according',\n",
              " 'account',\n",
              " 'accountant',\n",
              " 'accumulated',\n",
              " 'accumulates',\n",
              " 'accuracy',\n",
              " 'accurate',\n",
              " 'accurately',\n",
              " 'accuse',\n",
              " 'ace',\n",
              " 'acerbic',\n",
              " 'ache',\n",
              " 'achero',\n",
              " 'achieve',\n",
              " 'achieved',\n",
              " 'achievement',\n",
              " 'achievements',\n",
              " 'achieves',\n",
              " 'achieving',\n",
              " 'achingly',\n",
              " 'achival',\n",
              " 'acid',\n",
              " 'acidic',\n",
              " 'acidity',\n",
              " 'ackerman',\n",
              " 'acknowledges',\n",
              " 'acknowledging',\n",
              " 'acolytes',\n",
              " 'acquainted',\n",
              " 'acquire',\n",
              " 'acquired',\n",
              " 'acquires',\n",
              " 'acres',\n",
              " 'acrid',\n",
              " 'across',\n",
              " 'act',\n",
              " 'acted',\n",
              " 'acting',\n",
              " 'acting-workshop',\n",
              " 'action',\n",
              " 'action-adventure',\n",
              " 'action-and-popcorn',\n",
              " 'action-comedy',\n",
              " 'action-movie',\n",
              " 'action-oriented',\n",
              " 'action-packed',\n",
              " 'action-thriller\\\\/dark',\n",
              " 'action\\\\/comedy',\n",
              " 'action\\\\/thriller',\n",
              " 'actioner',\n",
              " 'actioners',\n",
              " 'actions',\n",
              " 'activate',\n",
              " 'activism',\n",
              " 'activists',\n",
              " 'activities',\n",
              " 'activity',\n",
              " 'actor',\n",
              " 'actor\\\\/director',\n",
              " 'actorish',\n",
              " 'actorly',\n",
              " 'actors',\n",
              " 'actory',\n",
              " 'actress',\n",
              " 'actress-producer',\n",
              " 'actresses',\n",
              " 'acts',\n",
              " 'actual',\n",
              " 'actually',\n",
              " 'actuary',\n",
              " 'acumen',\n",
              " 'acute',\n",
              " 'ad',\n",
              " 'adage',\n",
              " 'adam',\n",
              " 'adamant',\n",
              " 'adams',\n",
              " 'adaptation',\n",
              " 'adaptations',\n",
              " 'adapted',\n",
              " 'adapts',\n",
              " 'add',\n",
              " 'addams',\n",
              " 'added',\n",
              " 'addict',\n",
              " 'addicted',\n",
              " 'addiction',\n",
              " 'addictive',\n",
              " 'adding',\n",
              " 'addition',\n",
              " 'address',\n",
              " 'addresses',\n",
              " 'addressing',\n",
              " 'adds',\n",
              " 'adequate',\n",
              " 'adequately',\n",
              " 'adhere',\n",
              " 'adherents',\n",
              " 'adjective',\n",
              " 'adjusting',\n",
              " 'admirable',\n",
              " 'admirably',\n",
              " 'admiration',\n",
              " 'admire',\n",
              " 'admirer',\n",
              " 'admirers',\n",
              " 'admiring',\n",
              " 'admission',\n",
              " 'admit',\n",
              " 'admitted',\n",
              " 'admittedly',\n",
              " 'admitting',\n",
              " 'ado',\n",
              " 'adolescence',\n",
              " 'adolescent',\n",
              " 'adolescents',\n",
              " 'adopt',\n",
              " 'adopts',\n",
              " 'adorable',\n",
              " 'adorably',\n",
              " 'adored',\n",
              " 'adoring',\n",
              " 'adorns',\n",
              " 'adrenaline',\n",
              " 'adrenalized',\n",
              " 'adrian',\n",
              " 'adrien',\n",
              " 'adrift',\n",
              " 'adroit',\n",
              " 'adult',\n",
              " 'adults',\n",
              " 'advance',\n",
              " 'advantage',\n",
              " 'advantages',\n",
              " 'adventues',\n",
              " 'adventure',\n",
              " 'adventures',\n",
              " 'adventurous',\n",
              " 'adversity',\n",
              " 'advert',\n",
              " 'advertised',\n",
              " 'advertisement',\n",
              " 'advice',\n",
              " 'advised',\n",
              " 'advises',\n",
              " 'advocacy',\n",
              " 'aesop',\n",
              " 'aesthetic',\n",
              " 'aesthetics',\n",
              " 'affable',\n",
              " 'affair',\n",
              " 'affect',\n",
              " 'affectation',\n",
              " 'affectation-free',\n",
              " 'affected',\n",
              " 'affecting',\n",
              " 'affection',\n",
              " 'affectionate',\n",
              " 'affectionately',\n",
              " 'affections',\n",
              " 'affects',\n",
              " 'affinity',\n",
              " 'affirm',\n",
              " 'affirmational',\n",
              " 'affirming',\n",
              " 'affirms',\n",
              " 'affleck',\n",
              " 'afflicts',\n",
              " 'affluent',\n",
              " 'afford',\n",
              " 'affords',\n",
              " 'afghan',\n",
              " 'aficionados',\n",
              " 'afloat',\n",
              " 'afraid',\n",
              " 'africa',\n",
              " 'african',\n",
              " 'african-american',\n",
              " 'african-americans',\n",
              " 'after',\n",
              " 'after-school',\n",
              " 'afterlife',\n",
              " 'aftermath',\n",
              " 'afternoon',\n",
              " 'afterschool',\n",
              " 'aftertaste',\n",
              " 'afterthought',\n",
              " 'again',\n",
              " 'against',\n",
              " 'age',\n",
              " 'age-inspired',\n",
              " 'aged',\n",
              " 'agency',\n",
              " 'agenda',\n",
              " 'agendas',\n",
              " 'agent',\n",
              " 'agents',\n",
              " 'ages',\n",
              " 'ages-old',\n",
              " 'aggravating',\n",
              " 'aggressive',\n",
              " 'aggressively',\n",
              " 'aggressiveness',\n",
              " 'agile',\n",
              " 'aging',\n",
              " 'agitator',\n",
              " 'agitprop',\n",
              " 'ago',\n",
              " 'agonizing',\n",
              " 'agony',\n",
              " 'agreeably',\n",
              " 'agreed',\n",
              " 'aground',\n",
              " 'ah',\n",
              " 'ah-nuld',\n",
              " 'ahead',\n",
              " 'ahem',\n",
              " 'ahola',\n",
              " 'ai',\n",
              " 'aid',\n",
              " 'aids',\n",
              " 'ailments',\n",
              " 'aim',\n",
              " 'aimed',\n",
              " 'aimless',\n",
              " 'aimlessly',\n",
              " 'aimlessness',\n",
              " 'aims',\n",
              " 'air',\n",
              " 'air-conditioning',\n",
              " 'aircraft',\n",
              " 'airhead',\n",
              " 'airless',\n",
              " 'airs',\n",
              " 'airy',\n",
              " 'aisle',\n",
              " 'aisles',\n",
              " 'akin',\n",
              " 'al',\n",
              " 'alabama',\n",
              " 'alacrity',\n",
              " 'aladdin',\n",
              " 'alagna',\n",
              " 'alain',\n",
              " 'alan',\n",
              " 'alarming',\n",
              " 'alas',\n",
              " 'albeit',\n",
              " 'album',\n",
              " 'alcatraz',\n",
              " 'alchemical',\n",
              " 'alert',\n",
              " 'alexander',\n",
              " 'alexandre',\n",
              " 'alfonso',\n",
              " 'alfred',\n",
              " 'ali',\n",
              " 'alias',\n",
              " 'alice',\n",
              " 'alien',\n",
              " 'alienate',\n",
              " 'alienated',\n",
              " 'alienating',\n",
              " 'alienation',\n",
              " 'aliens',\n",
              " 'alike',\n",
              " 'alive',\n",
              " 'all',\n",
              " 'all-around',\n",
              " 'all-enveloping',\n",
              " 'all-inclusive',\n",
              " 'all-male',\n",
              " 'all-night',\n",
              " 'all-over-the-map',\n",
              " 'all-powerful',\n",
              " 'all-star',\n",
              " 'all-time',\n",
              " 'all-too-familiar',\n",
              " 'all-too-human',\n",
              " 'alleged',\n",
              " 'allegedly',\n",
              " 'allegiance',\n",
              " 'allegory',\n",
              " 'allen',\n",
              " 'allied',\n",
              " 'allow',\n",
              " 'allowed',\n",
              " 'allowing',\n",
              " 'allows',\n",
              " 'alluring',\n",
              " 'allusions',\n",
              " 'ally',\n",
              " 'almodovar',\n",
              " 'almost',\n",
              " 'alone',\n",
              " 'along',\n",
              " 'alongside',\n",
              " 'aloof',\n",
              " 'already',\n",
              " 'already-shallow',\n",
              " 'also',\n",
              " 'altar',\n",
              " 'alter',\n",
              " 'alterations',\n",
              " 'alternate',\n",
              " 'alternately',\n",
              " 'alternates',\n",
              " 'alternating',\n",
              " 'alternative',\n",
              " 'alternatives',\n",
              " 'although',\n",
              " 'altman',\n",
              " 'altman-esque',\n",
              " 'altogether',\n",
              " 'always',\n",
              " 'am',\n",
              " 'amalgam',\n",
              " 'amari',\n",
              " 'amaro',\n",
              " 'amassed',\n",
              " 'amateur',\n",
              " 'amateurish',\n",
              " 'amateurishly',\n",
              " 'amaze',\n",
              " 'amazement',\n",
              " 'amazing',\n",
              " 'amazingly',\n",
              " 'ambience',\n",
              " 'ambiguities',\n",
              " 'ambiguity',\n",
              " 'ambiguous',\n",
              " 'ambition',\n",
              " 'ambitions',\n",
              " 'ambitious',\n",
              " 'ambitiously',\n",
              " 'ambivalence',\n",
              " 'ambivalent',\n",
              " 'amble',\n",
              " 'ambrose',\n",
              " 'amc',\n",
              " 'america',\n",
              " 'american',\n",
              " 'american-russian',\n",
              " 'american-style',\n",
              " 'americanized',\n",
              " 'americans',\n",
              " 'amiable',\n",
              " 'amiably',\n",
              " 'amicable',\n",
              " 'amid',\n",
              " 'amidst',\n",
              " 'amini',\n",
              " 'amir',\n",
              " 'amish',\n",
              " 'amnesiac',\n",
              " 'amok',\n",
              " 'among',\n",
              " 'amoral',\n",
              " 'amos',\n",
              " 'amount',\n",
              " 'amounts',\n",
              " 'amours',\n",
              " 'amp',\n",
              " 'ample',\n",
              " 'amuse',\n",
              " 'amused',\n",
              " 'amusedly',\n",
              " 'amusement',\n",
              " 'amusements',\n",
              " 'amuses',\n",
              " 'amusing',\n",
              " 'amy',\n",
              " 'an',\n",
              " 'ana',\n",
              " 'anachronistic',\n",
              " 'anakin',\n",
              " 'analgesic',\n",
              " 'analysis',\n",
              " 'analytical',\n",
              " 'analyze',\n",
              " 'anarchist',\n",
              " 'anarchists',\n",
              " 'anatomical',\n",
              " 'anchored',\n",
              " 'anchoring',\n",
              " 'anchors',\n",
              " 'ancient',\n",
              " 'ancillary',\n",
              " 'and',\n",
              " 'and-miss',\n",
              " 'and\\\\/or',\n",
              " 'anderson',\n",
              " 'andie',\n",
              " 'andrei',\n",
              " 'android',\n",
              " 'anecdote',\n",
              " 'anemic',\n",
              " 'angel',\n",
              " 'angela',\n",
              " 'angeles',\n",
              " 'angelina',\n",
              " 'angelique',\n",
              " 'angels',\n",
              " 'anger',\n",
              " 'angle',\n",
              " 'angles',\n",
              " 'angry',\n",
              " 'angst',\n",
              " 'anguish',\n",
              " 'anguished',\n",
              " 'animal',\n",
              " 'animals',\n",
              " 'animated',\n",
              " 'animated-movie',\n",
              " 'animation',\n",
              " 'animations',\n",
              " 'animaton',\n",
              " 'animator',\n",
              " 'animatronic',\n",
              " 'anime',\n",
              " 'aniston',\n",
              " 'ankle-deep',\n",
              " 'anna',\n",
              " 'annals',\n",
              " 'anne',\n",
              " 'anne-sophie',\n",
              " 'annex',\n",
              " 'annie',\n",
              " 'annie-mary',\n",
              " 'anniversary',\n",
              " 'annoyance',\n",
              " 'annoyed',\n",
              " 'annoying',\n",
              " 'annual',\n",
              " 'anomaly',\n",
              " 'anomie',\n",
              " 'anonymity',\n",
              " 'anonymous',\n",
              " 'another',\n",
              " 'answer',\n",
              " 'answered',\n",
              " 'answering',\n",
              " 'answers',\n",
              " 'ante',\n",
              " 'anteing',\n",
              " 'anthology',\n",
              " 'anthony',\n",
              " 'anthropology',\n",
              " 'anthropomorphic',\n",
              " 'anti',\n",
              " 'anti-adult',\n",
              " 'anti-catholic',\n",
              " 'anti-erotic',\n",
              " 'anti-establishment',\n",
              " 'anti-feminist',\n",
              " 'anti-harry',\n",
              " 'anti-human',\n",
              " 'anti-kieslowski',\n",
              " 'anti-semitism',\n",
              " 'anti-virus',\n",
              " 'anti-war',\n",
              " 'antic',\n",
              " 'anticipated',\n",
              " 'antics',\n",
              " 'antidote',\n",
              " 'antique',\n",
              " 'antiseptic',\n",
              " 'antitrust',\n",
              " 'anton',\n",
              " 'antonia',\n",
              " 'antonio',\n",
              " 'ants',\n",
              " 'antsy',\n",
              " 'antwone',\n",
              " 'anxieties',\n",
              " 'anxious',\n",
              " 'any',\n",
              " 'anybody',\n",
              " 'anymore',\n",
              " 'anyone',\n",
              " 'anyplace',\n",
              " 'anything',\n",
              " 'anyway',\n",
              " 'anywhere',\n",
              " 'apart',\n",
              " 'apartheid',\n",
              " 'apartments',\n",
              " 'ape',\n",
              " 'apes',\n",
              " 'apex',\n",
              " 'aplenty',\n",
              " 'aplomb',\n",
              " 'apollo',\n",
              " 'apology',\n",
              " 'appalling',\n",
              " 'apparatus',\n",
              " 'apparent',\n",
              " 'apparently',\n",
              " 'appeal',\n",
              " 'appealing',\n",
              " 'appealingly',\n",
              " 'appeals',\n",
              " 'appear',\n",
              " 'appearance',\n",
              " 'appeared',\n",
              " 'appearing',\n",
              " 'appears',\n",
              " 'appetite',\n",
              " 'appetites',\n",
              " 'appetizer',\n",
              " 'appetizing',\n",
              " 'apple',\n",
              " 'applegate',\n",
              " 'applied',\n",
              " 'applies',\n",
              " 'apply',\n",
              " 'applying',\n",
              " 'appointed',\n",
              " 'appreciate',\n",
              " 'appreciated',\n",
              " 'appreciates',\n",
              " 'appreciation',\n",
              " 'appreciative',\n",
              " 'approach',\n",
              " 'approached',\n",
              " 'approaches',\n",
              " 'appropriate',\n",
              " 'appropriated',\n",
              " 'appropriately',\n",
              " 'april',\n",
              " 'apted',\n",
              " 'aptitude',\n",
              " 'aragorn',\n",
              " 'ararat',\n",
              " 'arbitrarily',\n",
              " 'arbitrary',\n",
              " 'arc',\n",
              " 'arcane',\n",
              " 'arch',\n",
              " 'archetypal',\n",
              " 'archibald',\n",
              " 'architect',\n",
              " 'architecture',\n",
              " 'archival',\n",
              " 'archives',\n",
              " 'arctic',\n",
              " 'ardent',\n",
              " 'ardently',\n",
              " 'ardor',\n",
              " 'arduous',\n",
              " 'are',\n",
              " 'area',\n",
              " 'areas',\n",
              " 'argentine',\n",
              " 'argentinean',\n",
              " 'argento',\n",
              " 'argot',\n",
              " 'arguably',\n",
              " 'argue',\n",
              " 'argues',\n",
              " 'arguing',\n",
              " 'argument',\n",
              " 'arguments',\n",
              " 'arising',\n",
              " 'aristocracy',\n",
              " 'aristocrat',\n",
              " 'aristocrats',\n",
              " 'arithmetic',\n",
              " 'ark',\n",
              " 'arliss',\n",
              " 'arm',\n",
              " 'armageddon',\n",
              " 'armed',\n",
              " 'armenian',\n",
              " 'armenians',\n",
              " 'arms',\n",
              " 'arnie',\n",
              " 'arnold',\n",
              " 'around',\n",
              " 'array',\n",
              " 'arrest',\n",
              " 'arresting',\n",
              " 'arrive',\n",
              " 'arrived',\n",
              " 'arrives',\n",
              " 'arriving',\n",
              " 'arrogance',\n",
              " 'arrow',\n",
              " 'art',\n",
              " 'art-house',\n",
              " 'artefact',\n",
              " 'arteta',\n",
              " 'artful',\n",
              " 'artfully',\n",
              " 'arthouse',\n",
              " 'arthur',\n",
              " 'articulate',\n",
              " 'articulates',\n",
              " 'artifice',\n",
              " 'artificial',\n",
              " 'artist',\n",
              " 'artistes',\n",
              " 'artistic',\n",
              " 'artistically',\n",
              " 'artistry',\n",
              " 'artists',\n",
              " 'artless',\n",
              " 'artnering',\n",
              " 'arts',\n",
              " 'artsploitation',\n",
              " 'artsy',\n",
              " 'artwork',\n",
              " 'artworks',\n",
              " 'arty',\n",
              " 'arwen',\n",
              " 'as',\n",
              " 'as-it',\n",
              " 'as-nasty',\n",
              " 'ascends',\n",
              " 'ascertain',\n",
              " 'ash',\n",
              " 'ashamed',\n",
              " 'ashley',\n",
              " 'asia',\n",
              " 'asian',\n",
              " 'asiaphiles',\n",
              " 'aside',\n",
              " 'ask',\n",
              " 'asked',\n",
              " 'asking',\n",
              " 'asks',\n",
              " 'asleep',\n",
              " 'asparagus',\n",
              " 'aspect',\n",
              " 'aspects',\n",
              " 'asphalt',\n",
              " 'aspiration',\n",
              " 'aspirations',\n",
              " 'aspire',\n",
              " 'aspired',\n",
              " 'aspires',\n",
              " 'asquith',\n",
              " 'assassin',\n",
              " 'assassination',\n",
              " 'assassins',\n",
              " 'assault',\n",
              " 'assaultive',\n",
              " 'assaults',\n",
              " 'assayas',\n",
              " 'assembled',\n",
              " 'assembly',\n",
              " 'assert',\n",
              " 'assess',\n",
              " 'assesses',\n",
              " 'asset',\n",
              " 'assets',\n",
              " 'assign',\n",
              " 'assigned',\n",
              " 'assignment',\n",
              " 'assimilated',\n",
              " 'associate',\n",
              " 'associated',\n",
              " 'association',\n",
              " 'associations',\n",
              " 'assume',\n",
              " 'assumes',\n",
              " 'assuming',\n",
              " 'assumption',\n",
              " 'assured',\n",
              " 'assuredly',\n",
              " 'assures',\n",
              " 'astonish',\n",
              " 'astonishing',\n",
              " 'astonishingly',\n",
              " 'astounding',\n",
              " 'astoundingly',\n",
              " 'astounds',\n",
              " 'astray',\n",
              " 'astringent',\n",
              " 'astronaut',\n",
              " 'astronauts',\n",
              " 'astute',\n",
              " 'asylum',\n",
              " 'at',\n",
              " 'at-a-frat-party',\n",
              " 'ate',\n",
              " 'athlete',\n",
              " 'athletes',\n",
              " 'athletic',\n",
              " 'athleticism',\n",
              " 'atlantic',\n",
              " 'atmosphere',\n",
              " 'atmospheric',\n",
              " 'atmospherics',\n",
              " 'atop',\n",
              " 'atrocious',\n",
              " 'atrociously',\n",
              " 'atrocities',\n",
              " 'attached',\n",
              " 'attachment',\n",
              " 'attack',\n",
              " 'attackers',\n",
              " 'attacks',\n",
              " 'attal',\n",
              " 'attempt',\n",
              " 'attempted',\n",
              " 'attempts',\n",
              " 'attendant',\n",
              " 'attending',\n",
              " 'attention',\n",
              " 'attentive',\n",
              " 'attics',\n",
              " 'attitude',\n",
              " 'attract',\n",
              " 'attracting',\n",
              " 'attraction',\n",
              " 'attractions',\n",
              " 'attractive',\n",
              " 'attracts',\n",
              " 'attributable',\n",
              " 'attuned',\n",
              " 'atypically',\n",
              " 'audacious',\n",
              " 'audacity',\n",
              " 'audiard',\n",
              " 'audience',\n",
              " 'audience-pleaser',\n",
              " 'audiences',\n",
              " 'auditorium',\n",
              " 'augmented',\n",
              " 'august',\n",
              " 'aurelie',\n",
              " 'auspicious',\n",
              " 'aussie',\n",
              " 'austen',\n",
              " 'austere',\n",
              " 'austerity',\n",
              " 'austin',\n",
              " 'australia',\n",
              " 'australian',\n",
              " 'auteil',\n",
              " 'auteuil',\n",
              " 'auteur',\n",
              " 'authentic',\n",
              " 'authentically',\n",
              " 'authenticity',\n",
              " 'author',\n",
              " 'authority',\n",
              " 'autistic',\n",
              " 'auto',\n",
              " 'auto-critique',\n",
              " 'autobiographical',\n",
              " 'autocritique',\n",
              " 'automatically',\n",
              " 'autopilot',\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WORLjMkSABKI",
        "colab_type": "text"
      },
      "source": [
        "**Everything in the function:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-nzwA3qYu4b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def neg_vectorizer(txt, data, col_names):\n",
        "    negations = ['not', 'no', 'none', 'never', 'neither', 'angry', 'annoy', 'anxious', 'annoying'\n",
        "              'awful', 'aggresive', 'afraid', 'bad', 'banal', 'awfully',\n",
        "              'boring', 'confused', 'crap', 'crazy', 'creepy', 'cruel', \n",
        "              'cry', 'damage', 'damaging', 'confusing', 'dead', 'depressed',\n",
        "              'dirty', 'disease', 'disgusting', 'disappointing', 'disappointment',\n",
        "              'difficult', 'disaster', 'disastrous', 'dangerous', 'dull', 'excuse',\n",
        "              'fail', 'fatal', 'fear', 'frighten', 'frightful', 'freaky', 'failed',\n",
        "              'grimace', 'grotesque', 'guilty', 'hard', 'harmful', 'hate', 'horrible',\n",
        "              'hurt', 'hurtful', 'horribly', 'icky', 'ignore', 'immature', 'imperfect', 'impossible', \n",
        "              'inelegant', 'insane', 'junky', 'lose', 'lousy', 'lumpy', 'lazy', 'little', 'zero', 'yell', \n",
        "              'worthless', 'ugly', 'unfair', 'unfavorable', 'unhappy', 'unlucky', \n",
        "              'unpleasant', 'unsatisfactory', 'unwanted','unwholesome', \n",
        "              'unwise', 'upset', 'unsuitable', 'unproffesional', 'uninteresting', 'terrible', 'terrifying',\n",
        "              'threatening', 'trouble', 'tedious', 'sad', 'scare', 'scary', 'scream', 'shocking', 'sick', 'sickening', 'sorry', \n",
        "              'stressful', 'stuck', 'stupid', 'silly', 'reject', 'rude', 'quit',\n",
        "              'questionable', 'mean', 'messy', 'missing', 'misunderstood',\n",
        "              'mistake', 'manipulation', 'mess', 'naive', 'negate', 'negative', 'nobody', \n",
        "              'nonsense', 'old', 'overrated', 'pain', 'pessimistic', 'poor', 'pumped']\n",
        "\n",
        "    full_lst = []\n",
        "    for sentence in txt:\n",
        "        full_lst.append(my_tokenizer(sentence))\n",
        "\n",
        "    lst_scores = []\n",
        "    for x in full_lst:\n",
        "        lst_scores.append([1]*len(x)) # na początek, każdemu wyrazowi w zdaniu przypisuję automatycznie 1 (=(-1)^0)\n",
        "\n",
        "    j = 0\n",
        "    for lst in full_lst:\n",
        "        l = len(lst)\n",
        "        for i in range(len(lst)-1):\n",
        "            if lst[i] in negations: # po negatywnym wyrazie zmieniamy znak 'score' wyrazów na prawo\n",
        "                lst_scores[j][i+1:] = [-1*x for x in lst_scores[j][i+1:]]\n",
        "        for k in range(len(lst)):\n",
        "            try:\n",
        "                # dodajemy scores do danego wiersza w tabeli wszystkich wyrazów\n",
        "                data.iloc[j][col_names.index(lst[k])] += lst_scores[j][k]\n",
        "            except ValueError:\n",
        "                pass\n",
        "        j += 1\n",
        "\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6B149EAOAYsO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_2 = neg_vectorizer(train_full['Phrase'].values, df(train_full['Phrase'], one_lst), one_lst)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJ5CQaJnA7pg",
        "colab_type": "code",
        "outputId": "c5c18298-9416-4020-c1cf-985def77a070",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.unique(train_data_2.values)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-5., -4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WrigC4nCaEp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_2 = neg_vectorizer(test_full['Phrase'].values, df(test_full['Phrase'], one_lst), one_lst)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRDLrwdYRlPK",
        "colab_type": "text"
      },
      "source": [
        "Logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpq13sENBZ8_",
        "colab_type": "code",
        "outputId": "ecea6272-7e11-4936-a5eb-294d1edad350",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "lr2 = LogisticRegression()\n",
        "lr2.fit(train_data_2.values, train_full['Sentiment'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsjHeZXoEWCp",
        "colab_type": "code",
        "outputId": "55237e54-d681-4ada-a2ae-4d60076bfd35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predicted2 = lr2.predict(test_data_2.values)\n",
        "np.mean(predicted2 == test_full['Sentiment'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3433616742969261"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjXVN9a4FAUg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cm2 = confusion_matrix(test_full['Sentiment'], predicted2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4fj80M9IKOh",
        "colab_type": "code",
        "outputId": "26ad4153-7517-483a-b1bd-d3346ff914e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "cm1 # previous model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 46,  86,  26,  27,   9],\n",
              "       [ 58, 177,  79,  57,  19],\n",
              "       [ 23,  84,  84,  85,  17],\n",
              "       [ 10,  67,  72, 200,  70],\n",
              "       [  4,  16,  27,  96,  90]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGGXht6eILOw",
        "colab_type": "code",
        "outputId": "a0d38b43-c7fd-45e0-b109-698b039b3356",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "cm2 # model with negations"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 37,  76,  33,  38,  10],\n",
              "       [ 49, 170,  76,  78,  17],\n",
              "       [ 23, 100,  63,  86,  21],\n",
              "       [ 19,  73,  74, 179,  74],\n",
              "       [  2,  20,  31, 104,  76]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuerqBm2CzEB",
        "colab_type": "text"
      },
      "source": [
        "It doesn't work better than CountVectorizer. The reason for this may be text preparation - I just used a simple tokenization. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7H7QsKZu4HKQ",
        "colab_type": "text"
      },
      "source": [
        "## 5.\n",
        "$$\\text{score}(\\text{sentence}) = \\sum_{w\\text{ in sentence}}N^{\\text{\\#negs prec. }w}E^{\\text{\\#emphs prec. }w}S_w$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jq4ZYDC44JFI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def neg_emphasize_vectorizer(txt, data, col_names, N, E):\n",
        "    negations = ['not', 'no', 'none', 'never', 'neither', 'angry', 'annoy', 'anxious', 'annoying'\n",
        "              'awful', 'aggresive', 'afraid', 'bad', 'banal', 'awfully',\n",
        "              'boring', 'confused', 'crap', 'crazy', 'creepy', 'cruel', \n",
        "              'cry', 'damage', 'damaging', 'confusing', 'dead', 'depressed',\n",
        "              'dirty', 'disease', 'disgusting', 'disappointing', 'disappointment',\n",
        "              'difficult', 'disaster', 'disastrous', 'dangerous', 'dull', 'excuse',\n",
        "              'fail', 'fatal', 'fear', 'frighten', 'frightful', 'freaky', 'failed',\n",
        "              'grimace', 'grotesque', 'guilty', 'hard', 'harmful', 'hate', 'horrible',\n",
        "              'hurt', 'hurtful', 'horribly', 'icky', 'ignore', 'immature', 'imperfect', 'impossible', \n",
        "              'inelegant', 'insane', 'junky', 'lose', 'lousy', 'lumpy', 'lazy', 'little', 'zero', 'yell', \n",
        "              'worthless', 'ugly', 'unfair', 'unfavorable', 'unhappy', 'unlucky', \n",
        "              'unpleasant', 'unsatisfactory', 'unwanted','unwholesome', \n",
        "              'unwise', 'upset', 'unsuitable', 'unproffesional', 'uninteresting', 'terrible', 'terrifying',\n",
        "              'threatening', 'trouble', 'tedious', 'sad', 'scare', 'scary', 'scream', 'shocking', 'sick', 'sickening', 'sorry', \n",
        "              'stressful', 'stuck', 'stupid', 'silly', 'reject', 'rude', 'quit',\n",
        "              'questionable', 'mean', 'messy', 'missing', 'misunderstood',\n",
        "              'mistake', 'manipulation', 'mess', 'naive', 'negate', 'negative', 'nobody', \n",
        "              'nonsense', 'old', 'overrated', 'pain', 'pessimistic', 'poor', 'pumped']\n",
        "    \n",
        "    emphasize = ['very', 'strongly', 'really', 'seriously', 'wonderful']\n",
        "\n",
        "    full_lst = []\n",
        "    for sentence in txt:\n",
        "        full_lst.append(my_tokenizer(sentence))\n",
        "\n",
        "    lst_scores = []\n",
        "    for x in full_lst:\n",
        "        lst_scores.append([1]*len(x)) # na początek, każdemu wyrazowi w zdaniu przypisuję automatycznie 1 (=(-1)^0)\n",
        "\n",
        "    j = 0\n",
        "    for lst in full_lst:\n",
        "        l = len(lst)\n",
        "        for i in range(len(lst)-1):\n",
        "            if lst[i] in emphasize: # po negatywnym wyrazie mnożymy wyrazy na prawo przez N, po uwydatnieniu przez E\n",
        "                lst_scores[j][i+1:] = [E*x for x in lst_scores[j][i+1:]]\n",
        "            elif lst[i] in negations:\n",
        "                lst_scores[j][i+1:] = [N*x for x in lst_scores[j][i+1:]]\n",
        "        for k in range(len(lst)):\n",
        "            try:\n",
        "                # dodajemy scores do danego wiersza w tabeli wszystkich wyrazów\n",
        "                data.iloc[j][col_names.index(lst[k])] += lst_scores[j][k]\n",
        "            except ValueError:\n",
        "                pass\n",
        "        j += 1\n",
        "\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbXBQMV5FKjW",
        "colab_type": "text"
      },
      "source": [
        "##### N = -1, E = 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMIkyuNs_LDK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_31 = neg_emphasize_vectorizer(train_full['Phrase'].values, df(train_full['Phrase'], one_lst), one_lst, -1, 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4WAbyY4_vdE",
        "colab_type": "code",
        "outputId": "f829a9de-413b-4e22-a92a-7532ba8a19ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "np.unique(train_data_31)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-27., -18., -10.,  -9.,  -6.,  -5.,  -4.,  -3.,  -2.,  -1.,   0.,\n",
              "         1.,   2.,   3.,   4.,   5.,   6.,   7.,   9.,  10.,  13.,  15.,\n",
              "        27.,  28.,  31.,  56.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmtaX0zM__H3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_31 = neg_emphasize_vectorizer(test_full['Phrase'].values, df(test_full['Phrase'], one_lst), one_lst, -1, 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X35lhVDlABoJ",
        "colab_type": "code",
        "outputId": "4b595e6a-1e40-41f7-bb85-9e77dafab3a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "lr31 = LogisticRegression()\n",
        "lr31.fit(train_data_31.values, train_full['Sentiment'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3tJ4APzASax",
        "colab_type": "code",
        "outputId": "6e8f733a-df93-430d-bd02-ea7632cffd73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predicted31 = lr31.predict(test_data_31.values)\n",
        "np.mean(predicted31 == test_full['Sentiment'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3433616742969261"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Hj6gncOAhLd",
        "colab_type": "code",
        "outputId": "8d71f64f-e14e-4f55-a37c-63484404bdec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "confusion_matrix(test_full['Sentiment'], predicted31)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 36,  73,  33,  42,  10],\n",
              "       [ 52, 169,  73,  78,  18],\n",
              "       [ 24,  98,  65,  86,  20],\n",
              "       [ 22,  73,  71, 184,  69],\n",
              "       [  4,  23,  30, 105,  71]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AI2hhJuGF42t",
        "colab_type": "text"
      },
      "source": [
        "##### N = -1, E = 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYsLV3VFFjbY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_32 = neg_emphasize_vectorizer(train_full['Phrase'].values, df(train_full['Phrase'], one_lst), one_lst, -1, 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLrL-pxYGHr0",
        "colab_type": "code",
        "outputId": "e34902c9-844c-4895-e7f1-802ced02aa8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "np.unique(train_data_32)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-125.,  -50.,  -26.,  -25.,  -20.,  -15.,  -10.,   -7.,   -6.,\n",
              "         -5.,   -4.,   -3.,   -2.,   -1.,    0.,    1.,    2.,    3.,\n",
              "          4.,    5.,    6.,    7.,    8.,   10.,   11.,   13.,   15.,\n",
              "         16.,   21.,   25.,   26.,   31.,  125.,  126.,  131.,  252.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvt4IFS9GJjb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_32 = neg_emphasize_vectorizer(test_full['Phrase'].values, df(test_full['Phrase'], one_lst), one_lst, -1, 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7tfDtwLGTz-",
        "colab_type": "code",
        "outputId": "68300dee-6ea3-4554-8a32-59957c4d1d1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "lr32 = LogisticRegression()\n",
        "lr32.fit(train_data_32.values, train_full['Sentiment'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQ2sj45KGdSu",
        "colab_type": "code",
        "outputId": "fde4a5c0-fe41-480e-e832-30b3f81e98ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predicted32 = lr32.predict(test_data_32.values)\n",
        "np.mean(predicted32 == test_full['Sentiment'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.34793982995421846"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVXX-er6Giaj",
        "colab_type": "code",
        "outputId": "d2df775d-63bb-43b1-832a-7f67beae823c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "confusion_matrix(test_full['Sentiment'], predicted32)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 35,  78,  31,  40,  10],\n",
              "       [ 50, 171,  75,  78,  16],\n",
              "       [ 25,  96,  67,  83,  22],\n",
              "       [ 23,  71,  71, 184,  70],\n",
              "       [  4,  24,  32,  98,  75]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-yrlvz-HgKm",
        "colab_type": "text"
      },
      "source": [
        "We have slightly better results in prediction of positive phrases (but still it's not a big difference). To change predictions for negative phrases, I'll try to change multiplier for negative words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjpJ31FJKHYO",
        "colab_type": "text"
      },
      "source": [
        "##### N = -3, E = 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLYFJKYOKJ-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_33 = neg_emphasize_vectorizer(train_full['Phrase'].values, df(train_full['Phrase'], one_lst), one_lst, -3, 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TVpFpCyKZcu",
        "colab_type": "code",
        "outputId": "edff8e26-0e14-4e8d-f9b7-b94689a7c264",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "np.unique(train_data_33)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-375., -150.,  -81.,  -78.,  -75.,  -70.,  -54.,  -53.,  -45.,\n",
              "        -32.,  -30.,  -27.,  -26.,  -24.,  -21.,  -20.,  -18.,  -15.,\n",
              "        -14.,  -13.,  -12.,  -11.,  -10.,   -9.,   -8.,   -6.,   -5.,\n",
              "         -4.,   -3.,   -2.,   -1.,    0.,    1.,    2.,    3.,    4.,\n",
              "          5.,    6.,    7.,    8.,    9.,   10.,   11.,   12.,   13.,\n",
              "         15.,   16.,   18.,   19.,   24.,   25.,   26.,   27.,   30.,\n",
              "         31.,   36.,   39.,   42.,   45.,   46.,   50.,   61.,   81.,\n",
              "         90.,  125.,  126.,  131.,  135.,  225.,  252.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meNOR450LRnK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_33 = neg_emphasize_vectorizer(test_full['Phrase'].values, df(test_full['Phrase'], one_lst), one_lst, -3, 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SojR7eB4LfBR",
        "colab_type": "code",
        "outputId": "b0db4883-e58d-41fd-ab01-3e09bbcfb184",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "lr33 = LogisticRegression()\n",
        "lr33.fit(train_data_33.values, train_full['Sentiment'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnNAaBkML1K2",
        "colab_type": "code",
        "outputId": "9495dbe9-a6b5-430e-d312-192ab722984c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predicted33 = lr33.predict(test_data_33.values)\n",
        "np.mean(predicted33 == test_full['Sentiment'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3198168737737083"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGWeEZJfL9sJ",
        "colab_type": "code",
        "outputId": "78cd282c-4cd6-4539-f10b-625ca02bff80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "confusion_matrix(test_full['Sentiment'], predicted33)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 27,  74,  32,  42,  19],\n",
              "       [ 45, 144,  81,  98,  22],\n",
              "       [ 21,  84,  67,  89,  32],\n",
              "       [ 20,  75,  74, 176,  74],\n",
              "       [  7,  24,  33,  94,  75]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQr2Y6IrMbfH",
        "colab_type": "text"
      },
      "source": [
        "All in all, emphasizing the words improves the model, but changing the multiplier for negations don't."
      ]
    }
  ]
}